import os
from typing import List
from dotenv import load_dotenv
from nicegui import ui
from orchestrator import load_config, build_bundle, ReviewLoop, LangGraphReviewLoop, ProvidersBundle
import base64
from io import BytesIO

# Load .env if present
load_dotenv()

HOST = os.getenv("HOST", "127.0.0.1")
PORT = int(os.getenv("PORT", "8888"))

cfg = load_config()  # config.yaml or example

def build_with_overrides(main_provider: str, main_model: str, main_temp: float, main_max: int, main_stop: List[str],
                         judge_provider: str, judge_model: str, judge_temp: float, judge_max: int, judge_stop: List[str]) -> ProvidersBundle:
            finally:
                # Stop auto-updating status when done
                stop_status_updates()
                # Final status update
                display_graph().0.1")
PORT = int(os.getenv("PORT", "8888"))

cfg = load_config()  # config.yaml or example

def build_with_overrides(main_provider: str, main_model: str, main_temp: float, main_max: int, main_stop: List[str],
                         judge_provider: str, judge_model: str, judge_temp: float, judge_max: int, judge_stop: List[str]) -> ProvidersBundle:
    base = load_config()
    base["main_ai"] = {
        "provider": main_provider, "model": main_model,
        "temperature": float(main_temp), "max_tokens": int(main_max),
        "stop": list(main_stop or []),
    }
    base["judge_ai"] = {
        "provider": judge_provider, "model": judge_model,
        "temperature": float(judge_temp), "max_tokens": int(judge_max),
        "stop": list(judge_stop or []),
    }
    return build_bundle(base)

ui.dark_mode().enable()
ui.page_title('AI Verify Loop (NiceGUI)')

ui.markdown('# AI Verify Loop (NiceGUI)').classes('text-2xl font-bold')

with ui.card().classes('w-full'):
    ui.markdown('### Configuration')
    with ui.row().classes('w-full gap-4'):
        # Workflow selection
        with ui.column().classes('flex-none'):
            workflow_type = ui.select(['original', 'langgraph'], value='langgraph', label='Workflow Type')
            refresh_models_btn = ui.button('Refresh Models', color='gray')
    
    with ui.grid(columns=4).classes('w-full gap-3'):
        main_provider = ui.select(['ollama', 'openai'], value=cfg.get('main_ai', {}).get('provider', 'ollama'), label='Main provider')
        main_model = ui.select([], label='Main model')
        main_temp = ui.number(label='Main temperature', value=cfg.get('main_ai', {}).get('temperature', 0.2), step=0.1, min=0, max=2)
        main_tokens = ui.number(label='Main max tokens', value=cfg.get('main_ai', {}).get('max_tokens', 2000), step=100, min=256, max=8192)
        main_stop = ui.input(label='Main stop tokens (comma-separated)', placeholder='</final>,<|end_of_reply|>')

        judge_provider = ui.select(['ollama', 'openai'], value=cfg.get('judge_ai', {}).get('provider', 'ollama'), label='Judge provider')
        judge_model = ui.select([], label='Judge model')
        judge_temp = ui.number(label='Judge temperature', value=cfg.get('judge_ai', {}).get('temperature', 0.1), step=0.1, min=0, max=2)
        judge_tokens = ui.number(label='Judge max tokens', value=cfg.get('judge_ai', {}).get('max_tokens', 1500), step=100, min=256, max=8192)
        judge_stop = ui.input(label='Judge stop tokens (comma-separated)', placeholder='</s>')

# Model management functions
def get_available_models(provider: str) -> List[str]:
    """Get available models for a provider"""
    if provider == 'ollama':
        from providers import get_ollama_models
        return get_ollama_models()
    elif provider == 'openai':
        return ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo', 'o1-preview', 'o1-mini']
    return []

def update_model_options():
    """Update model dropdown options based on selected providers"""
    try:
        # Update main model options
        main_models = get_available_models(main_provider.value)
        main_model.options = main_models
        if main_models and not main_model.value in main_models:
            main_model.value = main_models[0]
        
        # Update judge model options
        judge_models = get_available_models(judge_provider.value)
        judge_model.options = judge_models
        if judge_models and not judge_model.value in judge_models:
            judge_model.value = judge_models[0]
            
        ui.notify(f'Models updated: {len(main_models)} main, {len(judge_models)} judge', type='positive')
    except Exception as e:
        ui.notify(f'Error updating models: {e}', type='negative')

# Set up event handlers for provider changes
main_provider.on('change', update_model_options)
judge_provider.on('change', update_model_options)
refresh_models_btn.on('click', update_model_options)

# Initialize models on startup
update_model_options()

with ui.card().classes('w-full'):
    ui.markdown('### Task Prompt')
    # ‚¨áÔ∏è odstranƒõn unsupported argument auto_rows; nech√°me jen .props('autogrow')
    user_prompt = ui.textarea(label='Describe your task', placeholder='What should the Main AI produce?')
    user_prompt.props('autogrow')
    with ui.row():
        start_btn = ui.button('Run Review Loop', color='primary')
        finalize_btn = ui.button('Finalize', color='secondary')
        export_btn = ui.button('Export Transcript', color='gray')

with ui.card().classes('w-full'):
    ui.markdown('### Your Feedback (optional)')
    user_feedback = ui.textarea(label='What should be improved?')
    user_feedback.props('autogrow')
    iterate_btn = ui.button('Refine with User Feedback', color='primary')

# Po vytvo≈ôen√≠ tlaƒç√≠tek je hned znep≈ô√≠stupni
for b in (finalize_btn, export_btn, iterate_btn):
    try:
        b.disable()  # NiceGUI 2.x
    except Exception:
        b.props('disable')  # fallback pro star≈°√≠ verze

# Graph Visualization Section
with ui.card().classes('w-full'):
    ui.markdown('### Workflow Graph')
    with ui.row().classes('w-full gap-4'):
        graph_format = ui.select(['image', 'mermaid', 'ascii'], value='image', label='Visualization Format')
        refresh_graph_btn = ui.button('Refresh Graph', color='secondary')
    
    with ui.column().classes('w-full'):
        graph_container = ui.column().classes('w-full')

with ui.card().classes('w-full'):
    ui.markdown('### Chat')
    with ui.scroll_area().classes('h-96 w-full p-4'):
        chat_container = ui.column().classes('w-full gap-2')

session_loop: ReviewLoop | None = None
langgraph_loop: LangGraphReviewLoop | None = None
current_bundle: ProvidersBundle | None = None

def add(role: str, text: str, meta: dict | None = None):
    name = {'you':'You','main':'Main AI','judge':'Judge AI','orchestrator':'Orchestrator'}.get(role, role)
    
    # Truncate long messages and add expand functionality
    MAX_CHARS = 512
    is_truncated = len(text) > MAX_CHARS
    truncated_text = text[:MAX_CHARS] + "..." if is_truncated else text
    
    with chat_container:
        if is_truncated:
            # Create a container for the expandable message
            with ui.column().classes('w-full gap-1'):
                # Show truncated message initially
                message_container = ui.column().classes('w-full')
                with message_container:
                    ui.chat_message(truncated_text, name=name, sent=role=='you').props('dense')
                
                # Add expand/collapse button
                def toggle_message():
                    message_container.clear()
                    with message_container:
                        if toggle_btn.text == 'Show More':
                            ui.chat_message(text, name=name, sent=role=='you').props('dense')
                            toggle_btn.text = 'Show Less'
                        else:
                            ui.chat_message(truncated_text, name=name, sent=role=='you').props('dense')
                            toggle_btn.text = 'Show More'
                
                toggle_btn = ui.button('Show More', on_click=toggle_message).props('size=sm flat dense').classes('self-end text-xs')
        else:
            # Regular message for short text
            ui.chat_message(text, name=name, sent=role=='you').props('dense')
        
        # Add metadata if present
        if meta:
            ui.chat_message(str(meta), name='Meta', sent=False).props('dense').classes('text-xs opacity-70')

def parse_csv(s: str) -> List[str]:
    if not s: return []
    return [x.strip() for x in s.split(',') if x.strip()]

def display_graph():
    """Display the current workflow graph"""
    global langgraph_loop, current_bundle
    
    graph_container.clear()
    
    if not langgraph_loop and workflow_type.value == 'langgraph':
        # Build bundle first if needed
        try:
            bundle = build_with_overrides(
                main_provider.value, main_model.value, main_temp.value, main_tokens.value, parse_csv(main_stop.value),
                judge_provider.value, judge_model.value, judge_temp.value, judge_tokens.value, parse_csv(judge_stop.value),
            )
            langgraph_loop = LangGraphReviewLoop(bundle)
        except Exception as e:
            with graph_container:
                ui.markdown(f'**Error building graph:** {e}')
            return
    
    if langgraph_loop and workflow_type.value == 'langgraph':
        try:
            format_val = graph_format.value
            
            # Get current status
            status = langgraph_loop.get_current_status()
            
            with graph_container:
                # Show current status with role information
                if status['is_active']:
                    status_text = f"**üîÑ Currently Active:** {status['current_node']} (Iteration {status.get('iteration', 0)})"
                    if status.get('roles_switched'):
                        status_text += f" **üîÑ ROLES SWITCHED** since iteration {status.get('switch_iteration', 'N/A')}"
                    ui.markdown(status_text)
                elif status['is_completed']:
                    status_text = f"**‚úÖ Workflow Completed** (Final iteration: {status.get('iteration', 0)})"
                    if status.get('roles_switched'):
                        status_text += f" (Roles were switched)"
                    ui.markdown(status_text)
                else:
                    ui.markdown("**‚è≥ Workflow not started**")
                
                # Show current model assignments
                if status.get('main_model') and status.get('judge_model'):
                    role_indicator = " üîÑ" if status.get('roles_switched') else ""
                    ui.markdown(f"**Current Roles{role_indicator}:** Main AI: `{status['main_model']}` | Judge AI: `{status['judge_model']}`")
                
                if format_val == 'image':
                    # Get PNG bytes and display
                    png_bytes = langgraph_loop.get_graph_image()
                    # Convert bytes to base64 for display
                    import base64
                    b64_str = base64.b64encode(png_bytes).decode()
                    ui.html(f'<img src="data:image/png;base64,{b64_str}" style="max-width: 100%; height: auto;">')
                elif format_val == 'mermaid':
                    mermaid_code = langgraph_loop.get_graph_visualization('mermaid')
                    ui.code(mermaid_code, language='mermaid').classes('w-full')
                    ui.markdown('[Copy to mermaid.live for rendering](https://mermaid.live)')
                elif format_val == 'ascii':
                    ascii_graph = langgraph_loop.get_graph_visualization('ascii')
                    ui.code(ascii_graph, language='text').classes('w-full')
        except Exception as e:
            with graph_container:
                ui.markdown(f'**Error displaying graph:** {e}')
    else:
        with graph_container:
            ui.markdown('*Graph visualization available for LangGraph workflow only*')

async def on_start():
    global session_loop, langgraph_loop, current_bundle
    chat_container.clear()
    
    try:
        # Build the configuration bundle
        current_bundle = build_with_overrides(
            main_provider.value, main_model.value, main_temp.value, main_tokens.value, parse_csv(main_stop.value),
            judge_provider.value, judge_model.value, judge_temp.value, judge_tokens.value, parse_csv(judge_stop.value),
        )
        
        user_task = (user_prompt.value or '').strip()
        if not user_task:
            ui.notify('Please enter a task prompt.', type='warning')
            return
        
        if workflow_type.value == 'langgraph':
            # Use LangGraph workflow
            langgraph_loop = LangGraphReviewLoop(current_bundle)
            
            # Set up real-time chat callback
            langgraph_loop.set_chat_callback(add)
            
            add('orchestrator', 'Starting LangGraph workflow...')
            
            # Start auto-updating status
            start_status_updates()
            
            try:
                # Run the complete workflow - messages will appear in real-time via callback
                final_state = await langgraph_loop.run_workflow(user_task)
                add('orchestrator', f'Workflow completed after {final_state["iteration"]} iterations.')
            finally:
                # Stop auto-updating status when done
                stop_status_updates()
                # Final status update
                display_graph()
            
        else:
            # Use original workflow
            session_loop = ReviewLoop(current_bundle)
            add('orchestrator', 'Running Main AI‚Ä¶')
            main_ans = await session_loop.run_main(user_task)
            add('main', main_ans)

            add('orchestrator', 'Running Judge AI‚Ä¶')
            judge_json = await session_loop.run_judge()
            add('judge', str(judge_json), {'parsed': judge_json})

            add('orchestrator', 'Refining with Judge feedback‚Ä¶')
            refined = await session_loop.refine()
            add('main', refined, {'phase': 'refined'})

        iterate_btn.enable()
        finalize_btn.enable()
        export_btn.enable()
        ui.notify('Review cycle complete.', type='positive')
        
        # Update graph display
        display_graph()
        
    except Exception as e:
        ui.notify(f'Error: {e}', type='negative')

async def on_iterate():
    global session_loop, langgraph_loop
    
    fb = (user_feedback.value or '').strip()
    if not fb:
        ui.notify('Please type some feedback.', type='warning')
        return
    
    if workflow_type.value == 'langgraph' and langgraph_loop:
        try:
            add('you', f'(feedback) {fb}')
            add('orchestrator', 'Running refinement with feedback...')
            
            # Re-run workflow with user feedback
            user_task = (user_prompt.value or '').strip()
            final_state = await langgraph_loop.run_workflow(user_task, fb)
            
            # Display new transcript entries
            for entry in final_state['transcript']:
                # Only show new entries (simple heuristic)
                if entry not in [t for t in final_state['transcript'] if t['role'] != 'you']:
                    add(entry['role'], entry['content'], entry.get('meta', {}))
            
            user_feedback.value = ''
            ui.notify('Refinement complete.', type='positive')
            
        except Exception as e:
            ui.notify(f'Error: {e}', type='negative')
            
    elif session_loop:
        try:
            add('you', f'(feedback) {fb}')
            add('orchestrator', 'Judge re-evaluates latest answer‚Ä¶')
            judge_json = await session_loop.run_judge()
            add('judge', str(judge_json), {'parsed': judge_json})

            add('orchestrator', 'Refining with your feedback + Judge critique‚Ä¶')
            refined = await session_loop.refine(user_feedback.value)
            add('main', refined, {'phase': 'refined'})
            user_feedback.value = ''
        except Exception as e:
            ui.notify(f'Error: {e}', type='negative')
    else:
        ui.notify('No active session.', type='warning')

async def on_finalize():
    global session_loop, langgraph_loop
    
    if workflow_type.value == 'langgraph' and langgraph_loop:
        # LangGraph workflow already handles finalization
        add('orchestrator', 'Workflow already finalized in previous steps.')
        ui.notify('Workflow already completed.', type='info')
    elif session_loop:
        try:
            add('orchestrator', 'Finalizing deliverable‚Ä¶')
            final_text = await session_loop.finalize()
            add('main', final_text, {'phase': 'final'})
            ui.notify('Final deliverable generated.', type='positive')
        except Exception as e:
            ui.notify(f'Error: {e}', type='negative')
    else:
        ui.notify('No active session.', type='warning')

async def on_export():
    global session_loop, langgraph_loop
    
    transcript = None
    
    if workflow_type.value == 'langgraph' and langgraph_loop:
        # Get transcript from the stored state
        if hasattr(langgraph_loop, 'last_state') and langgraph_loop.last_state:
            transcript = langgraph_loop.last_state['transcript']
        else:
            ui.notify('No workflow state to export.', type='warning')
            return
    elif session_loop:
        transcript = session_loop.export()
    else:
        ui.notify('No active session.', type='warning')
        return
    
    import json
    data = json.dumps(transcript, ensure_ascii=False, indent=2)
    ui.download(data.encode(), filename='transcript.json')

def on_refresh_graph():
    """Handler for refresh graph button"""
    display_graph()

# P≈ôipojen√≠ handler≈Ø (m√≠sto dekor√°tor≈Ø)
start_btn.on('click', on_start)
iterate_btn.on('click', on_iterate)
finalize_btn.on('click', on_finalize)
export_btn.on('click', on_export)
refresh_graph_btn.on('click', on_refresh_graph)

# Auto-update timer for status
status_timer = None

def start_status_updates():
    """Start periodic status updates"""
    global status_timer
    if status_timer:
        status_timer.cancel()
    status_timer = ui.timer(2.0, lambda: display_graph())  # Update every 2 seconds

def stop_status_updates():
    """Stop periodic status updates"""
    global status_timer
    if status_timer:
        status_timer.cancel()
        status_timer = None

# Initialize graph display
display_graph()

ui.markdown(
    '---\n*Tip: For OpenAI set `OPENAI_API_KEY` (.env). For Ollama run `ollama serve` & `ollama pull <model>`.*'
).classes('text-sm text-gray-400')

if __name__ == '__main__':
    ui.run(host=HOST, port=PORT, reload=False)
